{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def join_source_schema_by_sourceid(schema):\n",
    "    sourceid = schema[schema['Path'].str.find('SourceId')>=0]\n",
    "    # sourceid.to_csv('test')\n",
    "    # print(sourceid)\n",
    "    ValueScore = schema[schema['Path'].str.endswith('Value')]\n",
    "    ValueScore = ValueScore[['ObjectId', 'Path', 'Value']]\n",
    "    \n",
    "    sourceid = sourceid[['ObjectId', 'Path', 'Value']]\n",
    "    sourceid['extra'] = sourceid['Path'].apply(lambda st: st[0:st.find('/Sources')])\n",
    "    # sourceid.to_csv('test')\n",
    "    # t = t[t['Path'].str.find('SourceId') >= 0]\n",
    "    ValueScore['extra'] = ValueScore['Path'].apply(lambda st: st[0:st.find('/Value')])\n",
    "    # ValueScore.to_csv('test')\n",
    "    # sourceid.to_csv('test')\n",
    "    \n",
    "    join = pd.merge(ValueScore,sourceid,how=\"left\", on=['ObjectId','extra'])\n",
    "    join = join.drop(labels=['extra','Path_y'], axis=1)\n",
    "    join = join.rename(columns={'Value_y':'SourceId','Path_x':'Path','Value_x':'Value'})\n",
    "    join['SourceId'] = join['SourceId'].fillna('null')\n",
    "    join = join[join['SourceId'] != 'null']    \n",
    "    join = join[join['Value'] != 'true']\n",
    "    print(join)\n",
    "    join.to_csv('test')\n",
    "    # join = pd.merge(join,source,how=\"left\", left_on='SourceId',right_on='ObjectId')\n",
    "\n",
    "    # join = join.drop(labels=['ObjectId_y'], axis=1)\n",
    "    # return join\n",
    "\n",
    "env = pd.read_csv('/usr/yubomai/esg/schemeB/env',error_bad_lines=False)\n",
    "join_source_schema_by_sourceid(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "schema = pd.read_csv('/usr/yubomai/esg/schemeB/env', error_bad_lines=False)\n",
    "schema = schema[schema['Path'].str.find('AsReported')>=0 and schema['Path'].endswith('Value')]\n",
    "schema = schema[schema['Value'].notna()]\n",
    "schema.to_csv('reported_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_schema(schema):\n",
    "    print('find all source id')\n",
    "    sourceid = schema[schema['Path'].str.endswith('SourceId')]\n",
    "    sourceid = sourceid[sourceid['Value'].notna()]\n",
    "    sourceid['extra'] = sourceid['Path'].apply(lambda st: st[0:st.find('/SourceId')])\n",
    "    # sourceid.to_csv('test')\n",
    "    print('find as report data')\n",
    "    # print(sourceid)\n",
    "    asreport = schema[schema['Path'].str.find('AsReported') >=0]\n",
    "    asreport = asreport[asreport['Path'].str.endswith('Value')]\n",
    "    asreport['extra'] = asreport['Path'].apply(lambda st: st[0:st.find('/AsReported[')] + '/Sources[{}]'.format(st[st.find('/AsReported[') + 12]))\n",
    "    # asreport.to_csv('test')\n",
    "    print('join asreport and sourceid')\n",
    "    join = pd.merge(asreport,sourceid,how=\"left\", on=['ObjectId','extra','StatementDetails/OrganizationId','StatementDetails/FinancialPeriodFiscalYear'])\n",
    "    join = join.drop(labels=['extra','Path_y'], axis=1)\n",
    "    join = join.rename(columns={'Value_y':'SourceId','Path_x':'Path','Value_x':'Value'})\n",
    "    # join['SourceId'] = join['SourceId'].astype(str)\n",
    "    return join\n",
    "    # ValueScore = ValueScore[['ObjectId', 'Path', 'Value']]\n",
    "    \n",
    "    # sourceid = sourceid[['ObjectId', 'Path', 'Value']]\n",
    "    # sourceid['extra'] = sourceid['Path'].apply(lambda st: st[0:st.find('/Sources')])\n",
    "    # # sourceid.to_csv('test')\n",
    "    # # t = t[t['Path'].str.find('SourceId') >= 0]\n",
    "    # ValueScore['extra'] = ValueScore['Path'].apply(lambda st: st[0:st.find('/Value')])\n",
    "    # # ValueScore.to_csv('test')\n",
    "    # # sourceid.to_csv('test')\n",
    "    \n",
    "    # join = pd.merge(ValueScore,sourceid,how=\"left\", on=['ObjectId','extra'])\n",
    "    # join = join.drop(labels=['extra','Path_y'], axis=1)\n",
    "    # join = join.rename(columns={'Value_y':'SourceId','Path_x':'Path','Value_x':'Value'})\n",
    "    # join['SourceId'] = join['SourceId'].fillna('null')\n",
    "    # join = join[join['SourceId'] != 'null']    \n",
    "    # join = join[join['Value'] != 'true']\n",
    "    # print(join)\n",
    "    # join.to_csv('test')\n",
    "    # join = pd.merge(join,source,how=\"left\", left_on='SourceId',right_on='ObjectId')\n",
    "\n",
    "    # join = join.drop(labels=['ObjectId_y'], axis=1)\n",
    "    # return join\n",
    "    \n",
    "def main(source_path, schema_path):\n",
    "    # read source\n",
    "    source = pd.read_csv(source_path,error_bad_lines=False)\n",
    "    SourceAbstract = source[source['Path'] == 'Sources/SourceAbstract'][['ObjectId','Value']]\n",
    "    SourcePublisher= source[source['Path'] == 'Sources/SourcePublisher'][['ObjectId','Value']]\n",
    "    SourceUrl= source[source['Path'] == 'Sources/SourceUrl'][['ObjectId','Value']]\n",
    "    SourceTitle = source[source['Path'] == 'Sources/SourceTitle'][['ObjectId','Value']]\n",
    "    SourceAbstract = pd.merge(SourceAbstract,SourcePublisher,how=\"left\", on=\"ObjectId\")\n",
    "    SourceAbstract = pd.merge(SourceAbstract,SourceUrl,how=\"left\", on=\"ObjectId\")\n",
    "    SourceAbstract = pd.merge(SourceAbstract,SourceTitle,how=\"left\", on=\"ObjectId\")\n",
    "    SourceAbstract.columns = ['ObjectId', 'Abstract', 'Publisher', 'Url', 'Title']\n",
    "    SourceAbstract['ObjectId'] = SourceAbstract['ObjectId'].astype(str)\n",
    "    # SourcePublisher.to_csv('test')\n",
    "\n",
    "    print('handle source done')\n",
    "    # read scheme\n",
    "    scheme = pd.read_csv(schema_path,error_bad_lines=False, keep_default_na=False)\n",
    "    scheme = format_schema(scheme)\n",
    "    scheme['SourceId'] = scheme['SourceId'].astype(str)\n",
    "    # print(SourcePublisher.info())\n",
    "    # print(scheme.info())\n",
    "    # scheme.to_csv('test')\n",
    "    print('handle schema done')\n",
    "    # join source and schema\n",
    "    res = pd.merge(scheme,SourceAbstract,how=\"left\", left_on='SourceId',right_on='ObjectId')\n",
    "    print('join source and schema done')\n",
    "\n",
    "    res = res[res['Url'].notna()]\n",
    "    res.to_csv('/data/env', index=False)\n",
    "    print('dump res done')\n",
    "    \n",
    "\n",
    "\n",
    "main('/usr/yubomai/esg/sources/all.csv', '/usr/yubomai/esg/schemeB/env')\n",
    "\n",
    "# env = pd.read_csv('/usr/yubomai/esg/schemeB/env',error_bad_lines=False,nrows=100000)\n",
    "# join_source_schema_by_sourceid(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res = pd.read_csv('/home/linzhisheng/esg/res',error_bad_lines=False)\n",
    "res = res[res['Url'].notna()]\n",
    "res.to_csv('env.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter table data\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "def is_chinese(string):\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_exponent(num):\n",
    "    return num.to_integral() if num == num.to_integral() else num.normalize()\n",
    "# def check_data(x):\n",
    "#     print(x[0])\n",
    "source = pd.read_csv('data/all')\n",
    "print(len(source))\n",
    "count = 0\n",
    "res = pd.DataFrame(columns=['value','text','type','path','url'])\n",
    "for value,text,type,url in zip(source['Value'],source['Abstract'], source['Path'],source['Url']):\n",
    "    path = type[type.find('/') + 1:type.find('/AsReported[')]\n",
    "    type = type[0:type.find('DataPoints')]\n",
    "    # print(type)\n",
    "    if int(value) == value:\n",
    "        value = int(value)\n",
    "    format_value = format(value,',')\n",
    "    # if text.find('\\t') < 0 and is_chinese(text) == False:\n",
    "    #     count = count + 1\n",
    "    if text.find(str(format_value)) >=0:\n",
    "        # res.loc[count] = [format_value,text,type]\n",
    "        \n",
    "        left = text.find(str(format_value))\n",
    "        right = text.find(str(format_value)) + len(str(format_value))\n",
    "        if left - 2 >=0 and text[left-2] !=' ' and right + 2 < len(text) and text[right+2] !=' ':\n",
    "            if not text[left-2].isdigit() and not text[right+2].isdigit():\n",
    "                res.loc[count] = [format_value,text,type,path,url]\n",
    "                count = count + 1\n",
    "        \n",
    "        \n",
    "    elif text.find(str(value)) >=0:\n",
    "        left = text.find(str(value))\n",
    "        right = text.find(str(value)) + len(str(value))\n",
    "        if left - 2 >=0 and text[left-2] !=' ' and right + 2 < len(text) and text[right+2] !=' ':\n",
    "            if not text[left-2].isdigit() and not text[right+2].isdigit():\n",
    "                res.loc[count] = [str(value),text,type,path,url]\n",
    "                count = count + 1\n",
    "    if count !=0 and count % 10000 ==0:\n",
    "        print(count)\n",
    "        # break\n",
    "print(count)\n",
    "print(count/len(source))\n",
    "res.to_csv('data/train_fine_grained_url.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "env = pd.read_csv('env')\n",
    "gov = pd.read_csv('gov')\n",
    "social = pd.read_csv('social')\n",
    "env = pd.concat([env,gov])\n",
    "env = pd.concat([env,social])\n",
    "env.to_csv('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'\n",
    "t.find('Saint Bernadette Soubirous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset('csv', data_files='data/train.csv')\n",
    "raw_datasets = raw_datasets['train'].train_test_split(test_size=0.1,seed=42)\n",
    "# print(raw_datasets['train'][0])\n",
    "raw_datasets['test'].to_csv('test.csv', index=False)\n",
    "# print(raw_datasets['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = 'bert-finetuned-esgQA-fine-grained-8-2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# text = '49, 100. 4'\n",
    "text = '49,100.4'\n",
    "input = tokenizer(text)\n",
    "print(input['input_ids'])\n",
    "print(tokenizer.decode(input['input_ids'], skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import re\n",
    "import json\n",
    "def question_set(example, template):\n",
    "    answer_set = example['answer_set']\n",
    "    id = 0\n",
    "    format_answer_set = {\n",
    "        'question':[],\n",
    "        'context':[],\n",
    "        'id':[]\n",
    "    }\n",
    "    for answer in answer_set:\n",
    "        if re.search(r'\\d', answer['answer']):\n",
    "            id = id + 1\n",
    "            format_answer_set['id'].append(str(id))\n",
    "            format_answer_set['question'].append(template.format(answer['answer']))\n",
    "            format_answer_set['context'].append(answer['context'].split('[SEP]')[1])\n",
    "\n",
    "    return Dataset.from_dict(format_answer_set)\n",
    "\n",
    "source = load_dataset('csv', data_files='data/figure.csv')\n",
    "print(source['train']['answer_set'][0])\n",
    "source = json.loads(source['train']['answer_set'][0])\n",
    "\n",
    "# print(question_set(source['train'],'What is {} about?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('http://www.onsemi.com/pub/Collateral/BRD8084-D.PDF')\n",
    "print(response.status_code)\n",
    "# with open('metadata.pdf', 'wb') as f:\n",
    "#     f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import pandas as pd\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/test_8_2_url.csv')\n",
    "# source = load_dataset('csv', data_files='/home/linzhisheng/esg/QA/data/test_8_2_url.csv')\n",
    "# source = source['train']\n",
    "source['aoligei'] = ''\n",
    "# source[0]['value'] = 'asd'\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "api = HfApi()\n",
    "filename = 'report_eng.csv'\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/linzhisheng/esg/QA/data/\" + filename,\n",
    "    path_in_repo=filename,\n",
    "    repo_id=\"JachinLin/esg_report\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model init success]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a9354f458d4029df\n",
      "Found cached dataset csv (/home/linzhisheng/.cache/huggingface/datasets/csv/default-a9354f458d4029df/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ab61662168404faecbab2169906670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9889fb7244504dc98833e550058e9faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c4ed000f3247018284c4eb486ab58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_validation_examples is :384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4985\n",
      "  Batch size = 256\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4985)\n",
      "=============[get_fiture_set]==============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/20 00:08 < 02:41, 0.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_22199/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3823596596.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">292</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_22199/3823596596.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_22199/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3823596596.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">132</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_fiture_set</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_22199/3823596596.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2862</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2859 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2860 │   │   </span>eval_loop = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prediction_loop <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.use_legacy_prediction_loop <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2861 │   │   </span>output = eval_loop(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2862 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>test_dataloader, description=<span style=\"color: #808000; text-decoration-color: #808000\">\"Prediction\"</span>, ignore_keys=ignore_keys, metric_k  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2863 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2864 │   │   </span>total_batch_size = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.eval_batch_size * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.world_size               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2865 │   │   </span>output.metrics.update(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2988</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluation_loop</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2985 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> nested_concat(inputs_host, inputs_decode, padding_index=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">100</span>)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2986 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2987 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> logits <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2988 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pad_across_processes(logits)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2989 │   │   │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._nested_gather(logits)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2990 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.preprocess_logits_for_metrics <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2991 │   │   │   │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.preprocess_logits_for_metrics(logits, labels)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3113</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_pad_across_processes</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3110 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">they can safely be gathered.</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3111 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3112 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensor)(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pad_across_processes(t, pad_index=pad_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3115 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensor)({k: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pad_across_processes(v, pad_index=pad_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">f</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, torch.Tensor):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3113</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;genexpr&gt;</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3110 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">they can safely be gathered.</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3111 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3112 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensor)(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pad_across_processes(t, pad_index=pad_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3115 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensor)({k: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pad_across_processes(v, pad_index=pad_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">f</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensor, torch.Tensor):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3124</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_pad_across_processes</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3121 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(tensor.shape) &lt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3122 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tensor                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3123 │   │   # Gather all sizes</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>size = torch.tensor(tensor.shape, device=tensor.device)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>]                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3125 │   │   </span>sizes = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._nested_gather(size).cpu()                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3126 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3127 │   │   </span>max_size = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(s[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> s <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> sizes)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_22199/\u001b[0m\u001b[1;33m3823596596.py\u001b[0m:\u001b[94m292\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_22199/3823596596.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_22199/\u001b[0m\u001b[1;33m3823596596.py\u001b[0m:\u001b[94m132\u001b[0m in \u001b[92mget_fiture_set\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_22199/3823596596.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2862\u001b[0m in \u001b[92mpredict\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2859 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2860 \u001b[0m\u001b[2m│   │   \u001b[0meval_loop = \u001b[96mself\u001b[0m.prediction_loop \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.use_legacy_prediction_loop \u001b[94melse\u001b[0m \u001b[96mse\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2861 \u001b[0m\u001b[2m│   │   \u001b[0moutput = eval_loop(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2862 \u001b[2m│   │   │   \u001b[0mtest_dataloader, description=\u001b[33m\"\u001b[0m\u001b[33mPrediction\u001b[0m\u001b[33m\"\u001b[0m, ignore_keys=ignore_keys, metric_k  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2863 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2864 \u001b[0m\u001b[2m│   │   \u001b[0mtotal_batch_size = \u001b[96mself\u001b[0m.args.eval_batch_size * \u001b[96mself\u001b[0m.args.world_size               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2865 \u001b[0m\u001b[2m│   │   \u001b[0moutput.metrics.update(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2988\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mevaluation_loop\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2985 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m nested_concat(inputs_host, inputs_decode, padding_index=-\u001b[94m100\u001b[0m)    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2986 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2987 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m logits \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2988 \u001b[2m│   │   │   │   \u001b[0mlogits = \u001b[96mself\u001b[0m._pad_across_processes(logits)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2989 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits = \u001b[96mself\u001b[0m._nested_gather(logits)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2990 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.preprocess_logits_for_metrics \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2991 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.preprocess_logits_for_metrics(logits, labels)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m3113\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_pad_across_processes\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3110 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mthey can safely be gathered.\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3111 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3112 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensor, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3113 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensor)(\u001b[96mself\u001b[0m._pad_across_processes(t, pad_index=pad_index) \u001b[94mfor\u001b[0m t  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensor, \u001b[96mdict\u001b[0m):                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3115 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensor)({k: \u001b[96mself\u001b[0m._pad_across_processes(v, pad_index=pad_index) \u001b[94mf\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(tensor, torch.Tensor):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m3113\u001b[0m in \u001b[92m<genexpr>\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3110 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mthey can safely be gathered.\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3111 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3112 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensor, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3113 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensor)(\u001b[96mself\u001b[0m._pad_across_processes(t, pad_index=pad_index) \u001b[94mfor\u001b[0m t  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensor, \u001b[96mdict\u001b[0m):                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3115 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensor)({k: \u001b[96mself\u001b[0m._pad_across_processes(v, pad_index=pad_index) \u001b[94mf\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(tensor, torch.Tensor):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/linzhisheng/esg-env/lib/python3.7/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m3124\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_pad_across_processes\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(tensor.shape) < \u001b[94m2\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3122 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m tensor                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3123 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Gather all sizes\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3124 \u001b[2m│   │   \u001b[0msize = torch.tensor(tensor.shape, device=tensor.device)[\u001b[94mNone\u001b[0m]                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3125 \u001b[0m\u001b[2m│   │   \u001b[0msizes = \u001b[96mself\u001b[0m._nested_gather(size).cpu()                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3126 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3127 \u001b[0m\u001b[2m│   │   \u001b[0mmax_size = \u001b[96mmax\u001b[0m(s[\u001b[94m1\u001b[0m] \u001b[94mfor\u001b[0m s \u001b[95min\u001b[0m sizes)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "from datasets import *\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "class ESG():\n",
    "    model = 0\n",
    "    tokenizer = 0\n",
    "    checkpoint = 0\n",
    "    trainer = 0\n",
    "    example_to_features = 0\n",
    "    start_logits = 0\n",
    "    end_logits = 0\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    top_n = 20\n",
    "    # predicted_answers = []\n",
    "    \n",
    "    def __init__(self, checkpoint):\n",
    "        self.checkpoint = checkpoint\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.args = TrainingArguments(\n",
    "            \"main_test\",\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=256,\n",
    "            fp16=True,\n",
    "            # no_cuda=True,\n",
    "            push_to_hub=False,\n",
    "            save_total_limit=1\n",
    "        )\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.args,\n",
    "            # train_dataset=train_dataset,\n",
    "            # eval_dataset=eval_set,\n",
    "            tokenizer=self.tokenizer,\n",
    "\n",
    "        )\n",
    "        print('[model init success]')\n",
    "\n",
    "    def update_model(self, checkpoint):\n",
    "        print('=============[update_model]==============')\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.args,\n",
    "            # train_dataset=train_dataset,\n",
    "            # eval_dataset=eval_set,\n",
    "            tokenizer=self.tokenizer,\n",
    "\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_model_result(self, example):\n",
    "        # print('=============[get_model_result]==============')\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        score = []\n",
    "        \n",
    "        for feature_index in self.example_to_features[example_id]:\n",
    "            # print((feature_index,tokenizer.decode(eval_set[\"input_ids\"][feature_index])))\n",
    "            start_logit = self.start_logits[feature_index]\n",
    "            end_logit = self.end_logits[feature_index]\n",
    "            offsets = self.offset_mapping[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(\n",
    "                start_logit)[-1: -self.n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -self.n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > self.max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    ans = context[offsets[start_index][0]: offsets[end_index][1]]\n",
    "                    if re.search(r'\\d', ans):\n",
    "                        score.append(start_logit[start_index] + end_logit[end_index])\n",
    "                        answers.append(\n",
    "                            {\n",
    "                                # \"id\": len(answers),\n",
    "                                \"answer\": ans,\n",
    "                                \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                                # \"context\": tokenizer.decode(self.eval_set[\"input_ids\"][feature_index], skip_special_tokens=False),\n",
    "                                # \"origin_text\": tokenizer.decode(eval_set[\"input_ids\"][feature_index])\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    # print((context[offsets[start_index][0] : offsets[end_index][1]], start_logit[start_index] + end_logit[end_index]))\n",
    "        from torch.nn.functional import softmax\n",
    "        score = torch.FloatTensor(score)\n",
    "        prob = softmax(score,dim=-1)\n",
    "        for i in range(len(answers)):\n",
    "            answers[i]['prob'] = prob[i].item()\n",
    "        \n",
    "        # best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        top_20_answers = heapq.nlargest(\n",
    "            self.top_n, answers, key=lambda s: s['prob'])\n",
    "        # predicted_answers.append(\n",
    "        #     {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
    "        example['answer_set'] = top_20_answers\n",
    "        return example\n",
    "        # print(top_20_answers)\n",
    "        \n",
    "    def get_fiture_set(self, small_eval_set, eval_set, top_n=20):\n",
    "        print('=============[get_fiture_set]==============')\n",
    "        self.top_n = top_n\n",
    "        self.eval_set = eval_set\n",
    "        predictions, _, _ = self.trainer.predict(eval_set)\n",
    "        self.start_logits, self.end_logits = predictions\n",
    "        self.offset_mapping = self.eval_set[\"offset_mapping\"]\n",
    "        self.example_to_features = collections.defaultdict(list)\n",
    "        for idx, feature in enumerate(eval_set):\n",
    "            self.example_to_features[feature[\"example_id\"]].append(idx)\n",
    "        \n",
    "        firuge_set = small_eval_set.map(self.get_model_result, batched=False)\n",
    "        return firuge_set\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "model_checkpoint = \"/home/linzhisheng/esg/QA/bert-finetuned-esgQA-fine-grained-8-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "id = 0\n",
    "\n",
    "\n",
    "def add_question_to_get_fiture(example):\n",
    "    global id\n",
    "    id = id + 1\n",
    "    example['question'] = \"How much {}?\".format(example['path'])\n",
    "    example['context'] = example['text']\n",
    "    # example['answers'] = {\n",
    "    #     'text': [example['value']],\n",
    "    #     'answer_start': [example['text'].find(example['value'])]\n",
    "    # }\n",
    "    example['id'] = str(id)\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_question(example, template):\n",
    "    answer_list = example['answer_set']\n",
    "    format_answer_list = []\n",
    "    for answer in answer_list:\n",
    "        if re.search(r'\\d', answer['text']):\n",
    "            format_answer_list.append(answer['text'])\n",
    "\n",
    "    global id\n",
    "    id = id + 1\n",
    "    example['question'] = \"How much {}?\".format(example['path'])\n",
    "    example['context'] = example['text']\n",
    "    # example['answers'] = {\n",
    "    #     'text': [example['value']],\n",
    "    #     'answer_start': [example['text'].find(example['value'])]\n",
    "    # }\n",
    "    example['id'] = str(id)\n",
    "    return example\n",
    "\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    print('preprocess_validation_examples is :{}'.format(max_length))\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "# input = input.replace('\\n',' ')\n",
    "\n",
    "# input = '''\n",
    "# The Group is principally engaged in education services. No substantial emissions are produced by\n",
    "# combustion of any fuels in daily operation as the Group is not engaged in any industrial production.\n",
    "# During the Reporting Period, the principal type of emission of the Group is exhaust generated by the\n",
    "# Group’s self-owned vehicles. The main emission data are as follows:\n",
    "# Major emissions Unit\n",
    "# Emission\n",
    "# volume\n",
    "# Nitrogen oxide (NOx) Gram 673,012.0\n",
    "# Sulphur dioxide (SOx) Gram 637.2\n",
    "# Particulate Matter Gram 66,032.1\n",
    "# '''\n",
    "\n",
    "def get_question_set(example, template):\n",
    "    answer_set = example['answer_set'][0]\n",
    "    id = 0\n",
    "    format_answer_set = {\n",
    "        'question':[],\n",
    "        'context':[],\n",
    "        'id':[]\n",
    "    }\n",
    "    # print(answer_set)\n",
    "    for answer in answer_set:\n",
    "        # print(answer)\n",
    "        id = id + 1\n",
    "        format_answer_set['id'].append(str(id))\n",
    "        format_answer_set['question'].append(template.format(answer['answer']))\n",
    "        # print(answer['context'].split('[SEP]'))\n",
    "        format_answer_set['context'].append(answer['context'].split('[SEP]')[1])\n",
    "\n",
    "    return Dataset.from_dict(format_answer_set)\n",
    "\n",
    "def question(eval_firuge_set, template):\n",
    "    next_question_set = get_question_set(eval_firuge_set, template)\n",
    "\n",
    "    eval_set = next_question_set.map(\n",
    "        preprocess_validation_examples,\n",
    "        batched=True,\n",
    "        remove_columns=next_question_set.column_names,\n",
    "    )\n",
    "    print((len(next_question_set),len(eval_set)))\n",
    "\n",
    "    result_set = extractor.get_fiture_set(next_question_set, eval_set, 5)\n",
    "    return result_set\n",
    "\n",
    "extractor = ESG(model_checkpoint)\n",
    "# get original data\n",
    "# example = {\n",
    "#     'text': [input,input,input],\n",
    "#     'path': ['ElectricityPurchased',\"EnergyUseTotal\", \"WasteRecycledTotal\"],\n",
    "# }\n",
    "# for i in range(10):\n",
    "#     example['text'].append(input)\n",
    "#     example['path'].append('emission')\n",
    "\n",
    "test_data = load_dataset('csv', data_files = '/home/linzhisheng/esg/QA/data/report_eng.csv')\n",
    "test_data = test_data['train'].select(range(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_data = Dataset.from_dict(example)\n",
    "small_eval_set = test_data.map(\n",
    "    add_question_to_get_fiture, remove_columns=test_data.column_names)\n",
    "\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=small_eval_set.column_names,\n",
    ")\n",
    "print((len(small_eval_set),len(eval_set)))\n",
    "\n",
    "eval_firuge_set = extractor.get_fiture_set(small_eval_set, eval_set, 20)\n",
    "eval_firuge_set = eval_firuge_set.remove_columns('context')\n",
    "eval_firuge_set.to_csv('figure.csv',index=False)\n",
    "\n",
    "# max_length = max_length + 50\n",
    "# extractor.update_model('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# result_set = question(eval_firuge_set,'What is the data {} about?')\n",
    "# result_set.to_csv('data/figure_about.csv',index=False)\n",
    "# # next question What is the unit of?\n",
    "# result_set = question(eval_firuge_set,'What is the unit of {}?')\n",
    "# result_set.to_csv('data/figure_unit.csv',index=False)\n",
    "# # next question What year?\n",
    "# result_set = question(eval_firuge_set,'What year is {} about?')\n",
    "# result_set.to_csv('data/figure_year.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27494/2262507953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0manswer_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "import pandas as pd\n",
    "import json\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/report_eng.csv')\n",
    "source = source.drop(columns=['text'])\n",
    "result = pd.read_csv('/home/linzhisheng/esg/QA/figure_1000_prob_all.csv')\n",
    "\n",
    "for T in range(10,20,10):\n",
    "    ALL = len(source)\n",
    "    cc = 0\n",
    "    count = 0\n",
    "    for i,row in source.iterrows():\n",
    "        target = row['value']\n",
    "        answer_set = result.iloc[i]['answer_set']\n",
    "        limit = 0\n",
    "        find = 0\n",
    "        for id,ii in enumerate(answer_set.split(\"{\\'answer\\': \")):\n",
    "            limit = limit + 1\n",
    "            if limit >= T + 2:\n",
    "                break\n",
    "            # print(ii)\n",
    "            ans = ii[1:ii.find('logit_score')-4]\n",
    "            # prob = ii[ii.find(\"'prob': \") + 8:-3]\n",
    "            # if prob.find('0.9') == 0 or prob.find('0.8') == 0:\n",
    "            #     print((count,target,ans, prob))\n",
    "            # if id == 1 and (prob.find('0.9') != 0):\n",
    "            #     # print((count,target,ans, prob))\n",
    "            #     ALL = ALL - 1\n",
    "            #     break\n",
    "                # print(id)\n",
    "            # print((prob))\n",
    "            # if ans.find(target) >=0 :\n",
    "            ans_no_space = ans.replace(' ','')\n",
    "            if ans_no_space != ans and ans_no_space == target:\n",
    "                count = count + 1\n",
    "                find = 1\n",
    "                break\n",
    "            if target =='0' or ans == target:\n",
    "                \n",
    "                count = count + 1\n",
    "                # print((count,target,ans, prob))\n",
    "                find = 1\n",
    "                break\n",
    "            # print((target,ans))\n",
    "        # if find == 0:\n",
    "        #     print((row['value'],row['path'],row['url']))\n",
    "        # print(answer_set[0])\n",
    "    print('{} {}'.format(T,count/(len(source))))\n",
    "    # print(count/ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '28', '0.27', '0.806845486164093')\n",
      "(1, '22', '1,915', '0.9209403395652771')\n",
      "(1, '0.15', '141.9', '0.8273787498474121')\n",
      "(1, '845,394', '71,180', '0.8430152535438538')\n",
      "(1, '3.95', '1,826', '0.9980701804161072')\n",
      "(1, '1500', '1,283,940', '0.9340410828590393')\n",
      "(1, '3,154', '56,000', '0.9989915490150452')\n",
      "(1, '7,500', '4,826,434', '0.9871166348457336')\n",
      "(1, '246,706', '3,665,258', '0.9408074617385864')\n",
      "(1, '43.5', '333,714', '0.9210011959075928')\n",
      "(1, '3,754', '450.7', '0.8621156811714172')\n",
      "(1, '3,523,194', '9,316,543', '0.8654746413230896')\n",
      "(1, '3.6', '4,500.18', '0.9837607741355896')\n",
      "(1, '1,073', '2.2', '0.9401983618736267')\n",
      "(1, '31,392.8', '39,587,200', '0.8455827832221985')\n",
      "(1, '4,472', '7,753,262', '0.8207216858863831')\n",
      "(1, '1,479,515', '538', '0.9959996938705444')\n",
      "(1, '16,950', '89', '0.9665095806121826')\n",
      "(1, '976', '87,696', '0.829297661781311')\n",
      "(1, '147,697', '1,910,212', '0.9569713473320007')\n",
      "(1, '40,700', '60,867,000', '0.8476575613021851')\n",
      "(1, '786', '1,579', '0.9991450309753418')\n",
      "(1, '1.65', '21,042,990', '0.9193570613861084')\n",
      "(1, '4.14', '42', '0.8799114227294922')\n",
      "(1, '54', '25,550', '0.8746669292449951')\n",
      "(1, '7,840', '079', '0.9376884698867798')\n",
      "(1, '1,037,800', '257.8', '0.9597581624984741')\n",
      "(1, '362,448.3', '185,000', '0.9320192933082581')\n",
      "(1, '1,467', '2,445,837', '0.9984450936317444')\n",
      "(1, '1.6', '4,947.48', '0.9995409250259399')\n",
      "(1, '3,761,916', '20', '0.9739537239074707')\n",
      "(1, '6.56', '18,798,569', '0.9588819146156311')\n",
      "(1, '17,000', '11,167 ,384', '0.9969922304153442')\n",
      "(1, '1,135', '3,464', '0.906963586807251')\n",
      "(1, '0.9', '110,203', '0.9263519644737244')\n",
      "(1, '1,906,603', '5,888', '0.9202772974967957')\n",
      "(1, '1.45', '196.54', '0.9446094632148743')\n",
      "(1, '0.06', '323,813', '0.860200047492981')\n",
      "(2, '120,087', '1,026', '0.9330756664276123')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27494/2040083930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0manswer_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# top递增统计，去除表格\n",
    "import pandas as pd\n",
    "import json\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/report_eng.csv')\n",
    "source = source.drop(columns=['text'])\n",
    "result = pd.read_csv('/home/linzhisheng/esg/QA/figure_1000_prob_all.csv')\n",
    "\n",
    "for T in range(10,20,10):\n",
    "    ALL = len(source)\n",
    "    cc = 0\n",
    "    count = 0\n",
    "    for i,row in source.iterrows():\n",
    "        target = row['value']\n",
    "        answer_set = result.iloc[i]['answer_set']\n",
    "        limit = 0\n",
    "        find = 0\n",
    "        for id,ii in enumerate(answer_set.split(\"{\\'answer\\': \")):\n",
    "            limit = limit + 1\n",
    "            if limit >= T + 2:\n",
    "                break\n",
    "            # print(ii)\n",
    "            ans = ii[1:ii.find('logit_score')-4]\n",
    "            prob = ii[ii.find(\"'prob': \") + 8:-3]\n",
    "            if prob.find('0.9') == 0 or prob.find('0.8') == 0:\n",
    "                print((count,target,ans, prob))\n",
    "            if id == 1 and (prob.find('0.9') != 0):\n",
    "                # print((count,target,ans, prob))\n",
    "                ALL = ALL - 1\n",
    "                break\n",
    "                # print(id)\n",
    "            # print((prob))\n",
    "            # if ans.find(target) >=0 :\n",
    "            ans_no_space = ans.replace(' ','')\n",
    "            if ans_no_space != ans and ans_no_space == target:\n",
    "                count = count + 1\n",
    "                find = 1\n",
    "                break\n",
    "            if target =='0' or ans == target:\n",
    "                \n",
    "                count = count + 1\n",
    "                # print((count,target,ans, prob))\n",
    "                find = 1\n",
    "                break\n",
    "            # print((target,ans))\n",
    "        # if find == 0:\n",
    "        #     print((row['value'],row['path'],row['url']))\n",
    "        # print(answer_set[0])\n",
    "    # print('{} {}'.format(T,count/(len(source)-107)))\n",
    "    print(count/ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1       value         type  \\\n",
      "1              1             2       1,031  ResourceUse   \n",
      "2              4            13          28    Workforce   \n",
      "3              5            22           0  ResourceUse   \n",
      "4              6            23  10,073,226  ResourceUse   \n",
      "5              7            25           3  ResourceUse   \n",
      "...          ...           ...         ...          ...   \n",
      "1145        1326          6261          33    Workforce   \n",
      "1146        1327          6263           1    Workforce   \n",
      "1147        1328          6267      11,343  ResourceUse   \n",
      "1148        1329          6280       1,614     Emission   \n",
      "1149        1331          6289          96     Emission   \n",
      "\n",
      "                                    path  \\\n",
      "1               RenewableEnergyPurchased   \n",
      "2                   AverageTrainingHours   \n",
      "3                    ElectricityProduced   \n",
      "4                   WaterWithdrawalTotal   \n",
      "5               RenewableEnergyPurchased   \n",
      "...                                  ...   \n",
      "1145                OccupationalDiseases   \n",
      "1146                   EmployeeAccidents   \n",
      "1147                      EnergyUseTotal   \n",
      "1148  CO2EquivalentsEmissionDirectScope1   \n",
      "1149                      HazardousWaste   \n",
      "\n",
      "                                                    url  \n",
      "1     https://www.corporate.carrier.com/Images/Corpo...  \n",
      "2     https://investors.puretechhealth.com/static-fi...  \n",
      "3     https://www.enablemidstream.com/wp-content/upl...  \n",
      "4     https://www.petradiamonds.com/wp-content/uploa...  \n",
      "5     https://www.edf.fr/sites/default/files/contrib...  \n",
      "...                                                 ...  \n",
      "1145  https://www.novartis.com/sites/www.novartis.co...  \n",
      "1146  https://www.manilawater.com/storage/files/12/i...  \n",
      "1147  https://www.vontobel.com/siteassets/about-vont...  \n",
      "1148  https://www.oxb.com/system/files/financial-rep...  \n",
      "1149  https://sustainability.slgreen.com/wp-content/...  \n",
      "\n",
      "[1043 rows x 6 columns]\n",
      "                                          question    id  \\\n",
      "1               How much RenewableEnergyPurchased?     2   \n",
      "2                   How much AverageTrainingHours?     3   \n",
      "3                    How much ElectricityProduced?     4   \n",
      "4                   How much WaterWithdrawalTotal?     5   \n",
      "5               How much RenewableEnergyPurchased?     6   \n",
      "...                                            ...   ...   \n",
      "1145                How much OccupationalDiseases?  1146   \n",
      "1146                   How much EmployeeAccidents?  1147   \n",
      "1147                      How much EnergyUseTotal?  1148   \n",
      "1148  How much CO2EquivalentsEmissionDirectScope1?  1149   \n",
      "1149                      How much HazardousWaste?  1150   \n",
      "\n",
      "                                             answer_set  \n",
      "1     [{'answer': '1,031', 'logit_score': 15.765625}...  \n",
      "2     [{'answer': '100', 'logit_score': 12.203125}\\n...  \n",
      "3     [{'answer': '1,563,018', 'logit_score': 18.484...  \n",
      "4     [{'answer': '40,179,468', 'logit_score': 15.49...  \n",
      "5     [{'answer': '195', 'logit_score': 15.203125}\\n...  \n",
      "...                                                 ...  \n",
      "1145  [{'answer': '33', 'logit_score': 13.484375}\\n ...  \n",
      "1146  [{'answer': '45852', 'logit_score': 15.9296875...  \n",
      "1147  [{'answer': '1,589.6', 'logit_score': 13.1875}...  \n",
      "1148  [{'answer': '1,614', 'logit_score': 16.5}\\n {'...  \n",
      "1149  [{'answer': '96', 'logit_score': 14.578125}\\n ...  \n",
      "\n",
      "[1043 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 筛选掉表格\n",
    "import pandas as pd\n",
    "import json\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/report_eng.csv')\n",
    "source = source.drop(columns=['text'])\n",
    "result = pd.read_csv('/home/linzhisheng/esg/QA/data/figure_1000.csv')\n",
    "source['value'] = source['value'].astype('str')\n",
    "topn = 1000\n",
    "count = 0\n",
    "for i,row in source.iterrows():\n",
    "    target = row['value']\n",
    "    answer_set = result.iloc[i]['answer_set']\n",
    "    limit = 0\n",
    "    find = 0\n",
    "    for ii in answer_set.split(\"{\\'answer\\': \"):\n",
    "        limit = limit + 1\n",
    "        if limit >= topn + 2:\n",
    "            break\n",
    "        ans = ii[1:ii.find('logit_score')-4]\n",
    "        # if ans.find(target) >=0 :\n",
    "        ans_no_space = ans.replace(' ','')\n",
    "        if ans_no_space != ans and ans_no_space == target:\n",
    "            count = count + 1\n",
    "            find = 1\n",
    "            break\n",
    "        if target =='0' or ans == target:\n",
    "            \n",
    "            count = count + 1\n",
    "            # print((count,target,ans))\n",
    "            find = 1\n",
    "            break\n",
    "        # print((target,ans))\n",
    "    if find == 0:\n",
    "        1\n",
    "        source.loc[i,'value'] = ''\n",
    "        result.loc[i,'answer_set'] = ''\n",
    "        # print((row['value'],row['path'],row['url']))\n",
    "    # print(answer_set[0])\n",
    "source = source[source['value'] != '']\n",
    "result = result[result['answer_set'] != '']\n",
    "# print(source[source['value'] != ''])\n",
    "# source = source[source['value'].str != '']\n",
    "source.to_csv('/home/linzhisheng/esg/QA/data/no_table.csv',index=False)\n",
    "result.to_csv('/home/linzhisheng/esg/QA/data/no_table_result.csv',index=False)\n",
    "print(source)\n",
    "print(result)\n",
    "# print('{}'.format(count/(len(source))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7277085330776606\n"
     ]
    }
   ],
   "source": [
    "# 查看非表格中匹配不上的数据\n",
    "import pandas as pd\n",
    "import json\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/no_table.csv')\n",
    "# source = source.drop(columns=['text'])\n",
    "result = pd.read_csv('/home/linzhisheng/esg/QA/data/no_table_result.csv')\n",
    "source['value'] = source['value'].astype('str')\n",
    "topn = 10\n",
    "count = 0\n",
    "source['answer'] = ''\n",
    "# check = pd.DataFrame(columns=['value', 'path', 'url', 'answer'], index=[0,1,2])\n",
    "\n",
    "for i,row in source.iterrows():\n",
    "    top_answer = ''\n",
    "    target = row['value']\n",
    "    answer_set = result.iloc[i]['answer_set']\n",
    "    limit = 0\n",
    "    find = 0\n",
    "    for ii in answer_set.split(\"{\\'answer\\': \"):\n",
    "        limit = limit + 1\n",
    "        if limit <= 1:\n",
    "            continue\n",
    "        if limit >= topn + 2:\n",
    "            break\n",
    "        ans = ii[1:ii.find('logit_score')-4]\n",
    "        # if ans.find(target) >=0 :\n",
    "        ans_no_space = ans.replace(' ','')\n",
    "        top_answer = top_answer + '[{}]'.format(ans) + ', '\n",
    "        if ans_no_space != ans and ans_no_space == target:\n",
    "            count = count + 1\n",
    "            find = 1\n",
    "            source.loc[i,'answer'] = top_answer\n",
    "            break\n",
    "        if target =='0' or ans == target:\n",
    "            \n",
    "            count = count + 1\n",
    "            # print((count,target,ans))\n",
    "            # print((row['value'],row['path'],row['url'],top_answer))\n",
    "            source.loc[i,'answer'] = top_answer\n",
    "            find = 1\n",
    "            break\n",
    "        # print((target,ans))\n",
    "    if find == 0:\n",
    "        1\n",
    "        # source.loc[i,'answer'] = top_answer\n",
    "        # result.loc[i,'answer_set'] = ''\n",
    "        \n",
    "        # print((row['value'],row['path'],row['url'],top_answer))\n",
    "        # print('\\n')\n",
    "    # print(answer_set[0])\n",
    "print('{}'.format(count/(len(source))))\n",
    "source = source[source['answer'] != '']\n",
    "# result = result[result['answer_set'] != '']\n",
    "# # print(source[source['value'] != ''])\n",
    "# # source = source[source['value'].str != '']\n",
    "source = source.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "source.to_excel('/home/linzhisheng/esg/QA/data/right.xlsx',index=False)\n",
    "# result.to_csv('/home/linzhisheng/esg/QA/data/no_table_result.csv',index=False)\n",
    "# print(source)\n",
    "# print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"path\": \"AverageTrainingHours\",\n",
      "  \"count\": 9,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"LostWorkingDays\",\n",
      "  \"count\": 5,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"LostTimeInjuryRateTotal\",\n",
      "  \"count\": 5,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"EmployeeLostWorkingDays\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"WaterRecycled\",\n",
      "  \"count\": 3,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"CoalProducedTotal\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"TurnoverOfEmployees\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnvironmentalRAndDExpenditures\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"AlcoholRevenues\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 1.0\n",
      " },\n",
      " {\n",
      "  \"path\": \"NumberOfEmployeesFromCsrReporting\",\n",
      "  \"count\": 24,\n",
      "  \"score\": 0.875\n",
      " },\n",
      " {\n",
      "  \"path\": \"ElectricityPurchased\",\n",
      "  \"count\": 82,\n",
      "  \"score\": 0.8414634146341463\n",
      " },\n",
      " {\n",
      "  \"path\": \"AccidentalSpills\",\n",
      "  \"count\": 6,\n",
      "  \"score\": 0.8333333333333334\n",
      " },\n",
      " {\n",
      "  \"path\": \"RenewableEnergyProduced\",\n",
      "  \"count\": 61,\n",
      "  \"score\": 0.8032786885245902\n",
      " },\n",
      " {\n",
      "  \"path\": \"TotalInjuryRateEmployees\",\n",
      "  \"count\": 41,\n",
      "  \"score\": 0.7804878048780488\n",
      " },\n",
      " {\n",
      "  \"path\": \"RenewableEnergyPurchased\",\n",
      "  \"count\": 31,\n",
      "  \"score\": 0.7741935483870968\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnergyUseTotal\",\n",
      "  \"count\": 93,\n",
      "  \"score\": 0.7741935483870968\n",
      " },\n",
      " {\n",
      "  \"path\": \"WasteTotal\",\n",
      "  \"count\": 25,\n",
      "  \"score\": 0.76\n",
      " },\n",
      " {\n",
      "  \"path\": \"TotalInjuryRateTotal\",\n",
      "  \"count\": 41,\n",
      "  \"score\": 0.7560975609756098\n",
      " },\n",
      " {\n",
      "  \"path\": \"LostTimeInjuryRateEmployees\",\n",
      "  \"count\": 8,\n",
      "  \"score\": 0.75\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnvironmentalProvisions\",\n",
      "  \"count\": 4,\n",
      "  \"score\": 0.75\n",
      " },\n",
      " {\n",
      "  \"path\": \"ContractorFatalities\",\n",
      "  \"count\": 4,\n",
      "  \"score\": 0.75\n",
      " },\n",
      " {\n",
      "  \"path\": \"ElectricityProduced\",\n",
      "  \"count\": 23,\n",
      "  \"score\": 0.7391304347826086\n",
      " },\n",
      " {\n",
      "  \"path\": \"EmployeeHealthAndSafetyTrainingHours\",\n",
      "  \"count\": 7,\n",
      "  \"score\": 0.7142857142857143\n",
      " },\n",
      " {\n",
      "  \"path\": \"CO2EquivalentsEmissionIndirectScope2\",\n",
      "  \"count\": 7,\n",
      "  \"score\": 0.7142857142857143\n",
      " },\n",
      " {\n",
      "  \"path\": \"TrainingHoursTotal\",\n",
      "  \"count\": 34,\n",
      "  \"score\": 0.7058823529411765\n",
      " },\n",
      " {\n",
      "  \"path\": \"NOxEmissions\",\n",
      "  \"count\": 19,\n",
      "  \"score\": 0.6842105263157895\n",
      " },\n",
      " {\n",
      "  \"path\": \"SOxEmissions\",\n",
      "  \"count\": 12,\n",
      "  \"score\": 0.6666666666666666\n",
      " },\n",
      " {\n",
      "  \"path\": \"CO2EquivalentsEmissionTotal\",\n",
      "  \"count\": 18,\n",
      "  \"score\": 0.6666666666666666\n",
      " },\n",
      " {\n",
      "  \"path\": \"EmployeeFatalities\",\n",
      "  \"count\": 6,\n",
      "  \"score\": 0.6666666666666666\n",
      " },\n",
      " {\n",
      "  \"path\": \"TotalInjuryRateContractors\",\n",
      "  \"count\": 6,\n",
      "  \"score\": 0.6666666666666666\n",
      " },\n",
      " {\n",
      "  \"path\": \"SalariesAndWagesFromCsrReporting\",\n",
      "  \"count\": 3,\n",
      "  \"score\": 0.6666666666666666\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnergyPurchasedDirect\",\n",
      "  \"count\": 50,\n",
      "  \"score\": 0.6\n",
      " },\n",
      " {\n",
      "  \"path\": \"WasteRecycledTotal\",\n",
      "  \"count\": 55,\n",
      "  \"score\": 0.6\n",
      " },\n",
      " {\n",
      "  \"path\": \"OccupationalDiseases\",\n",
      "  \"count\": 10,\n",
      "  \"score\": 0.6\n",
      " },\n",
      " {\n",
      "  \"path\": \"OzoneDepletingSubstances\",\n",
      "  \"count\": 5,\n",
      "  \"score\": 0.6\n",
      " },\n",
      " {\n",
      "  \"path\": \"HazardousWaste\",\n",
      "  \"count\": 32,\n",
      "  \"score\": 0.59375\n",
      " },\n",
      " {\n",
      "  \"path\": \"WaterDischarged\",\n",
      "  \"count\": 19,\n",
      "  \"score\": 0.5789473684210527\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnergyProducedDirect\",\n",
      "  \"count\": 21,\n",
      "  \"score\": 0.5714285714285714\n",
      " },\n",
      " {\n",
      "  \"path\": \"CO2EquivalentsEmissionDirectScope1\",\n",
      "  \"count\": 30,\n",
      "  \"score\": 0.5666666666666667\n",
      " },\n",
      " {\n",
      "  \"path\": \"WaterWithdrawalTotal\",\n",
      "  \"count\": 46,\n",
      "  \"score\": 0.5434782608695652\n",
      " },\n",
      " {\n",
      "  \"path\": \"EmployeeAccidents\",\n",
      "  \"count\": 13,\n",
      "  \"score\": 0.5384615384615384\n",
      " },\n",
      " {\n",
      "  \"path\": \"FreshWaterWithdrawalTotal\",\n",
      "  \"count\": 17,\n",
      "  \"score\": 0.5294117647058824\n",
      " },\n",
      " {\n",
      "  \"path\": \"DonationsTotal\",\n",
      "  \"count\": 48,\n",
      "  \"score\": 0.5\n",
      " },\n",
      " {\n",
      "  \"path\": \"EmissionReductionTargetYear\",\n",
      "  \"count\": 4,\n",
      "  \"score\": 0.5\n",
      " },\n",
      " {\n",
      "  \"path\": \"VocEmissions\",\n",
      "  \"count\": 6,\n",
      "  \"score\": 0.5\n",
      " },\n",
      " {\n",
      "  \"path\": \"BoardMemberTermDuration\",\n",
      "  \"count\": 2,\n",
      "  \"score\": 0.5\n",
      " },\n",
      " {\n",
      "  \"path\": \"CO2EquivalentsEmissionIndirectScope3\",\n",
      "  \"count\": 26,\n",
      "  \"score\": 0.4230769230769231\n",
      " },\n",
      " {\n",
      "  \"path\": \"TrainingCostsTotal\",\n",
      "  \"count\": 10,\n",
      "  \"score\": 0.4\n",
      " },\n",
      " {\n",
      "  \"path\": \"WaterPollutantEmissions\",\n",
      "  \"count\": 5,\n",
      "  \"score\": 0.4\n",
      " },\n",
      " {\n",
      "  \"path\": \"CarbonOffsetsCredits\",\n",
      "  \"count\": 6,\n",
      "  \"score\": 0.3333333333333333\n",
      " },\n",
      " {\n",
      "  \"path\": \"NonHazardousWaste\",\n",
      "  \"count\": 20,\n",
      "  \"score\": 0.3\n",
      " },\n",
      " {\n",
      "  \"path\": \"EnvironmentalExpenditures\",\n",
      "  \"count\": 11,\n",
      "  \"score\": 0.2727272727272727\n",
      " },\n",
      " {\n",
      "  \"path\": \"AverageEmployeeLengthOfService\",\n",
      "  \"count\": 4,\n",
      "  \"score\": 0.25\n",
      " },\n",
      " {\n",
      "  \"path\": \"PoliticalContributions\",\n",
      "  \"count\": 5,\n",
      "  \"score\": 0.2\n",
      " },\n",
      " {\n",
      "  \"path\": \"AccidentsTotal\",\n",
      "  \"count\": 9,\n",
      "  \"score\": 0.1111111111111111\n",
      " },\n",
      " {\n",
      "  \"path\": \"FlaringGases\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 0\n",
      " },\n",
      " {\n",
      "  \"path\": \"TradeUnionRepresentation\",\n",
      "  \"count\": 2,\n",
      "  \"score\": 0\n",
      " },\n",
      " {\n",
      "  \"path\": \"IndirectEnergyUse\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 0\n",
      " },\n",
      " {\n",
      "  \"path\": \"ContractorAccidents\",\n",
      "  \"count\": 2,\n",
      "  \"score\": 0\n",
      " },\n",
      " {\n",
      "  \"path\": \"TotalRenewableEnergy\",\n",
      "  \"count\": 1,\n",
      "  \"score\": 0\n",
      " }\n",
      "]\n",
      "0.6596356663470757\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 统计各个粒度下的准确率，筛选掉表格\n",
    "import pandas as pd\n",
    "import json\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/data/no_table.csv')\n",
    "# source = source.drop(columns=['text'])\n",
    "result = pd.read_csv('/home/linzhisheng/esg/QA/data/no_table_result.csv')\n",
    "\n",
    "import json\n",
    "count = 0\n",
    "table_num = 0\n",
    "topn = 5\n",
    "path_map = {}\n",
    "hit_map = {}\n",
    "res_map = []\n",
    "for i,row in source.iterrows():\n",
    "    path = row['path']\n",
    "    target = row['value']\n",
    "    if path in path_map:\n",
    "        path_map[path] = path_map[path] + 1\n",
    "    else:\n",
    "        path_map[path] = 1\n",
    "     \n",
    "    answer_set = result.iloc[i]['answer_set']\n",
    "    limit = 0\n",
    "    find = 0\n",
    "    # path_set.add(path)\n",
    "    # print((path,target))\n",
    "    # break\n",
    "    for ii in answer_set.split(\"{\\'answer\\': \"):\n",
    "        limit = limit + 1\n",
    "        if limit >= topn + 2:\n",
    "            break\n",
    "        ans = ii[1:ii.find('logit_score')-4]\n",
    "        # if ans.find(target) >=0 :\n",
    "        ans_no_space = ans.replace(' ','')\n",
    "        \n",
    "        if ans_no_space != ans and ans_no_space == target:\n",
    "            count = count + 1\n",
    "            find = 1\n",
    "            if path in hit_map:\n",
    "                hit_map[path] = hit_map[path] + 1\n",
    "            else:\n",
    "                hit_map[path] = 1\n",
    "            break\n",
    "            # print(ans_no_space)\n",
    "        if target =='0' or ans == target:\n",
    "            # print((count,target,ans))\n",
    "            count = count + 1\n",
    "            if path in hit_map:\n",
    "                hit_map[path] = hit_map[path] + 1\n",
    "            else:\n",
    "                hit_map[path] = 1\n",
    "            find = 1\n",
    "            break\n",
    "        \n",
    "        # print((target,ans))\n",
    "    # if find == 0:\n",
    "    #     table_num = table_num + 1\n",
    "    #     print((row['value'],row['path'],row['url']))\n",
    "    # print(answer_set[0])\n",
    "    \n",
    "for key in path_map:\n",
    "    if key not in hit_map:\n",
    "        res_map.append({\n",
    "            'path':key,\n",
    "            'count':path_map[key],\n",
    "            'score':0\n",
    "        })\n",
    "        # res_map[key] = (path_map[key],0)\n",
    "    else:\n",
    "        res_map.append({\n",
    "            'path':key,\n",
    "            'count':path_map[key],\n",
    "            'score':hit_map[key]/path_map[key]\n",
    "        })\n",
    "        # res_map[key] = (path_map[key],hit_map[key]/path_map[key])\n",
    "    \n",
    "import heapq\n",
    "\n",
    "# print(res_map)\n",
    "res = heapq.nlargest(100, res_map, key=lambda s: s['score'])\n",
    "print(json.dumps(res,indent=1))\n",
    "# print(json.dumps(res_map, indent=1))\n",
    "# print(json.dumps(hit_map, indent=2))\n",
    "print('{}'.format(count/len(source)))\n",
    "# print(table_num)\n",
    "print('{}'.format(table_num/len(source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.read_csv('/home/linzhisheng/esg/QA/data/report_eng.csv')\n",
    "s = s.drop(columns='text')\n",
    "s.to_csv('data/report_figure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import PyPDF2\n",
    "pdf = pdfplumber.open(\"/home/linzhisheng/esg/QA/data/shire-annual-report-2014-26-03-15.pdf\")\n",
    "first_page = pdf.pages[51]\n",
    "im=first_page.to_image()\n",
    "im\n",
    "table = first_page.extract_table()\n",
    "table\n",
    "# text = first_page.extract_text()\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /home/linzhisheng/esg/QA/bert-finetuned-esgQA-fine-grained-8-2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/linzhisheng/esg/QA/bert-finetuned-esgQA-fine-grained-8-2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/linzhisheng/esg/QA/bert-finetuned-esgQA-fine-grained-8-2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /home/linzhisheng/esg/QA/bert-finetuned-esgQA-fine-grained-8-2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model init success]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95aa4f85871743aeb56ccad1139bee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a5d5320b93437b920f513926761a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_validation_examples is :384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 98\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 98)\n",
      "=============[get_fiture_set]==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4710452cc264dbf8ae9293e16e3de99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb93774a33b745f3a82d0dced25a559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "source = pd.read_csv('/home/linzhisheng/esg/QA/report_all_eng.csv')\n",
    "source = source.drop(columns='text')\n",
    "source.to_csv('report_all_figure.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /home/linzhisheng/.cache/huggingface/datasets/csv/default-1eab69efdf6bce5a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5381002788b1dc69.arrow is neither a dataset directory nor a dataset dict directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27494/1175785589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/linzhisheng/.cache/huggingface/datasets/csv/default-1eab69efdf6bce5a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5381002788b1dc69.arrow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esg-env/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory)\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         raise FileNotFoundError(\n\u001b[0;32m-> 1763\u001b[0;31m             \u001b[0;34mf\"Directory {dataset_path} is neither a dataset directory nor a dataset dict directory.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m         )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory /home/linzhisheng/.cache/huggingface/datasets/csv/default-1eab69efdf6bce5a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-5381002788b1dc69.arrow is neither a dataset directory nor a dataset dict directory."
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "t = load_from_disk('/home/linzhisheng/.cache/huggingface/datasets/csv/default-1eab69efdf6bce5a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "705f550babcdce45f48d8787f58c81cb6391e873d59f11c8d7074ed5565f0fb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
